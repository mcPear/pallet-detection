{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import imutils\n",
    "import sys\n",
    "import numpy\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "%run commons.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'naive_bayes_model.sav'\n",
    "gnb = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "filename_blurred = 'blurred_naive_bayes_model.sav'\n",
    "gnb_blurred = pickle.load(open(filename_blurred, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sector detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "percentile = 80 # percentil, with rows we accept as pallets (with more black pixel than in other rows)\n",
    "frame = 10 # we decice that row is important if it neighbours in frame are also in percentil\n",
    "threshold = 5 # threshold of neihtbours in frame in detecting important rows\n",
    "border_size = 10 # we detecting groups of important rows (mayby pallet), \n",
    "                        #this parameter is maximal space between rows\n",
    "\n",
    "def get_pallet_sectors(img):\n",
    "    hist = [255 - np.mean(row) for row in img]\n",
    "    perc = np.percentile(hist, percentile)\n",
    "    hist_perc = [max(x - perc, 0) for x in hist]\n",
    "\n",
    "    def check_frame(row_id, image):\n",
    "        return np.count_nonzero(image[row_id: row_id + frame]) > threshold\n",
    "\n",
    "    detection = [ row_id for row_id, value in enumerate(hist_perc) if check_frame(row_id, hist_perc) ]\n",
    "\n",
    "    def check_row(list_id, rows):\n",
    "        if list_id == len(rows) -1 :\n",
    "            return rows[list_id] - rows[list_id - 1] < border_size\n",
    "        if list_id == 0:\n",
    "            return rows[list_id + 1] - rows[list_id] < border_size\n",
    "        return rows[list_id] - rows[list_id - 1] > border_size or rows[list_id + 1] - rows[list_id] > border_size\n",
    "        \n",
    "    borders = [x for x_id, x in enumerate(detection) if check_row(x_id, detection)]\n",
    "    up = borders[-2] - 4\n",
    "    bottom = borders[-1] + 4\n",
    "    return up, bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    " cv2.imshow('image',img)\n",
    " cv2.waitKey(0)\n",
    " cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detect(img, model):\n",
    "    w,h,ch = img.shape\n",
    "    f = features(img, channels, False)\n",
    "    pred = model.predict(f)\n",
    "    img_pred = np.reshape(pred, (w,h,1))\n",
    "    return np.logical_not(img_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_filter(img):\n",
    "    kernel = np.ones((5,5),np.float32)/25\n",
    "    img = cv2.filter2D(img,-1,kernel)\n",
    "    return cv2.threshold(img,0.5,1.0,cv2.THRESH_BINARY)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def rotate(img, angle):\n",
    "    return  1-imutils.rotate(1-img, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd(num):\n",
    "    return (num % 3) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pallet_by_height(img, mask_height):\n",
    "    mask_height = mask_height\n",
    "    mask_width = int(mask_height * 5.556)\n",
    "    mask_size = mask_height * mask_width\n",
    "    hole_height = int(mask_height * 0.694)\n",
    "    hole_width = int(mask_height * 1.58)\n",
    "    hole_size = hole_height * hole_width\n",
    "    hole_1_x = int(mask_height * 0.694)\n",
    "    hole_1_y = int(mask_height * 0.306)\n",
    "    hole_2_x = int(mask_height * 3.281)\n",
    "    hole_2_y = hole_1_y\n",
    "    best = (None, 0, (0,0))\n",
    "    inverse_img = np.logical_not(img)\n",
    "    img_height, img_width = inverse_img.shape\n",
    "    for index, x in np.ndenumerate(inverse_img):\n",
    "        x,y = index\n",
    "        if odd(x) and odd(y) and y+mask_height < img_height and x+mask_width < img_width:\n",
    "            frame_mask = inverse_img[y:y+mask_height, x:x+mask_width]\n",
    "            hole_1_y_ = y+hole_1_y\n",
    "            hole_2_y_ = y+hole_2_y\n",
    "            hole_1_x_ = x+hole_1_x\n",
    "            hole_2_x_ = x+hole_2_x\n",
    "            hole_1_mask = inverse_img[hole_1_y_:hole_1_y_+hole_height, hole_1_x_:hole_1_x_+hole_width]\n",
    "            hole_2_mask = inverse_img[hole_2_y_:hole_2_y_+hole_height, hole_2_x_:hole_2_x_+hole_width]\n",
    "            frame_mask_perc = np.sum(frame_mask) / mask_size\n",
    "            hole_1_mask_perc = np.sum(hole_1_mask) / hole_size\n",
    "            hole_2_mask_perc = np.sum(hole_2_mask) / hole_size\n",
    "            perc = frame_mask_perc - hole_1_mask_perc - hole_2_mask_perc\n",
    "            if perc >= best[1]:\n",
    "                best = (frame_mask, perc, (x,y))\n",
    "    return best\n",
    "\n",
    "def find_pallet(img, min_height, max_height, step, min_perc=0.0):\n",
    "    best = (None, min_perc, (0,0,0))\n",
    "    for mask_height in range(min_height, max_height, step):\n",
    "        (best_for_height, perc, (x,y)) = find_pallet_by_height(img, mask_height)\n",
    "        if perc >= best[1]:\n",
    "            print(\"height: \", mask_height, \"percent: \",perc)\n",
    "            best = (best_for_height, perc, (x,y,mask_height))\n",
    "    \n",
    "    return (np.logical_not(best[0]), best[2])\n",
    "\n",
    "def draw_pallet(img_full,x,y,mask_height):\n",
    "    print(x,y)\n",
    "    mask_width = int(mask_height * 5.556)\n",
    "    color = (0,255,0)\n",
    "    img_full[y:y+mask_height, x:x+1]=color\n",
    "    img_full[y:y+mask_height, x+mask_width-1:x+mask_width]=color\n",
    "    img_full[y:y+1, x:x+mask_width]=color\n",
    "    img_full[y+mask_height-1:y+mask_height, x:x+mask_width]=color\n",
    "    return img_full\n",
    "\n",
    "def calculate_center(x,y,mask_height):\n",
    "    mask_width = int(mask_height * 5.556)\n",
    "    x = x + mask_width/2\n",
    "    y = y + mask_height/2\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(img, file, binary=True):\n",
    "    res = cv2.imwrite(file, img * 255 if binary else img)\n",
    "    print(\"saved\" if res else \"save error\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoising(img, with_save):\n",
    "    img = 1-img\n",
    "    kernel = np.ones((7,7), np.uint8) \n",
    "    img_erosion = cv2.erode(img, kernel, iterations=1)\n",
    "    if with_save:\n",
    "        save(1-img_erosion, \"img_erosion.jpg\")\n",
    "    img_dilation = cv2.dilate(img_erosion, kernel, iterations=1)\n",
    "    if with_save:\n",
    "        save(1-img_dilation, \"img_dilation.jpg\")\n",
    "    return 1-img_dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved img_classified.jpg\n",
      "saved img_erosion.jpg\n",
      "saved img_dilation.jpg\n",
      "saved img_rotated.jpg\n",
      "height:  25 percent:  0.2299442586399108\n",
      "height:  28 percent:  0.2731914096090667\n",
      "height:  31 percent:  0.3656442384405625\n",
      "saved img_pallet.jpg\n",
      "222 255\n",
      "saved img_full_marked.jpg\n",
      "center:  (308.0, 270.5)  x,y: 222 255  mask_height:  31\n"
     ]
    }
   ],
   "source": [
    "img_full = cv2.imread('r_1_11.jpg')\n",
    "img_blurred = cv2.medianBlur(img_full, 7)\n",
    "\n",
    "img_classified = detect(img_blurred, gnb_blurred).astype('float32')\n",
    "save(img_classified, \"img_classified.jpg\")\n",
    "\n",
    "#img_filtered = median_filter(img_classified)\n",
    "#save(img_filtered, \"img_filtered.jpg\")\n",
    "img_denoised = denoising(img_classified, True)\n",
    "\n",
    "img_rotated = rotate(img_denoised, -2)\n",
    "save(img_rotated, \"img_rotated.jpg\")\n",
    "\n",
    "img_pallet, (x,y,mask_height) = find_pallet(img_rotated, 25, 100, 3)\n",
    "save(img_pallet, \"img_pallet.jpg\")\n",
    "\n",
    "img_full_marked = draw_pallet(img_full,x,y,mask_height)\n",
    "save(img_full_marked, \"img_full_marked.jpg\", False)\n",
    "center = calculate_center(x,y,mask_height)\n",
    "print(\"center: \",center,\" x,y:\",x,y,\" mask_height: \",mask_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline as function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localise(path,filename):\n",
    "    img_full = cv2.imread(path+filename)\n",
    "    #img_blurred = cv2.medianBlur(img_full, 7)\n",
    "    img_classified = detect(img_full, gnb).astype('float32')\n",
    "    img_denoised = denoising(img_classified, False)\n",
    "    #img_filtered = median_filter(img_classified)\n",
    "    img_rotated = rotate(img_denoised, -2)\n",
    "    img_pallet, (x,y,mask_height) = find_pallet(img_rotated, 25, 100, 3)\n",
    "    img_full_marked = draw_pallet(img_full,x,y,mask_height)\n",
    "    save(img_full_marked, path+\"localised/\"+filename, False)\n",
    "    return [calculate_center(x,y,mask_height)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localize_all():\n",
    "    centers = dict()\n",
    "    path = \"/home/maciej/repos/pallet-recognition/data/jpeg_marked/\"\n",
    "    filenames = [f for f in sorted(listdir(path)) if isfile(join(path, f))]\n",
    "    for i in tqdm(range(len(filenames))):\n",
    "        filename = filenames[i]\n",
    "        print(path, filename)\n",
    "        centers[filename]=localise(path,filename)\n",
    "    np.save(path+\"eval/pred_centers\", centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/85 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_0.jpg\n",
      "height:  25 percent:  0.28964259951472227\n",
      "height:  28 percent:  0.30975459175798736\n",
      "height:  31 percent:  0.37293177461031923\n",
      "height:  34 percent:  0.37401230106131433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/85 [00:16<23:02, 16.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402 252\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_0.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_1.jpg\n",
      "height:  25 percent:  0.250172470325923\n",
      "height:  28 percent:  0.3668765021057042\n",
      "height:  31 percent:  0.42536080448683594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 2/85 [00:32<22:37, 16.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225 249\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_1.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_10.jpg\n",
      "height:  25 percent:  0.2818637287690996\n",
      "height:  31 percent:  0.33162234010883673\n",
      "height:  34 percent:  0.3824063303082914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▎         | 3/85 [00:48<22:11, 16.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402 252\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_10.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_11.jpg\n",
      "height:  25 percent:  0.29507508689094364\n",
      "height:  28 percent:  0.35740855070226885\n",
      "height:  31 percent:  0.4437924957429834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▍         | 4/85 [01:04<21:49, 16.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 252\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_11.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_12.jpg\n",
      "height:  25 percent:  0.2708977637877894\n",
      "height:  28 percent:  0.28646396048772954\n",
      "height:  31 percent:  0.44117577013300946\n",
      "height:  34 percent:  0.450638795828666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 5/85 [01:20<21:32, 16.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 252\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_12.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_13.jpg\n",
      "height:  25 percent:  0.28825627910026885\n",
      "height:  28 percent:  0.2990507794412718\n",
      "height:  31 percent:  0.4216656842782124\n",
      "height:  34 percent:  0.4605790564703008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 6/85 [01:36<21:09, 16.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 252\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_13.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_14.jpg\n",
      "height:  25 percent:  0.2934094038953374\n",
      "height:  28 percent:  0.2959219897250458\n",
      "height:  31 percent:  0.353110599078341\n",
      "height:  37 percent:  0.40222444479803604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 7/85 [01:52<20:48, 16.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 255\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_14.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_15.jpg\n",
      "height:  25 percent:  0.304573414650141\n",
      "height:  40 percent:  0.3853456631234409\n",
      "height:  43 percent:  0.4552788475861446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|▉         | 8/85 [02:08<20:34, 16.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 255\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_15.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_16.jpg\n",
      "height:  25 percent:  0.2500334448160535\n",
      "height:  40 percent:  0.2890680362902585\n",
      "height:  43 percent:  0.40124996844301397\n",
      "height:  46 percent:  0.4487051856740827\n",
      "height:  49 percent:  0.45099176034049976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█         | 9/85 [02:24<20:12, 15.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 258\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_16.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_17.jpg\n",
      "height:  25 percent:  0.23635910551511574\n",
      "height:  46 percent:  0.25319670177561443\n",
      "height:  49 percent:  0.28408408818072683\n",
      "height:  52 percent:  0.3489241322701689\n",
      "height:  55 percent:  0.4168985620395216\n",
      "height:  58 percent:  0.44353098176186634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▏        | 10/85 [02:40<19:56, 15.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 261\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_17.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_18.jpg\n",
      "height:  25 percent:  0.32608695652173914\n",
      "height:  28 percent:  0.3721264304456155\n",
      "height:  67 percent:  0.39722440691752625\n",
      "height:  70 percent:  0.4258959253804615\n",
      "height:  73 percent:  0.4364017088360943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 11/85 [02:56<19:41, 15.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 267\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_18.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_19.jpg\n",
      "height:  25 percent:  0.28698931077447704\n",
      "height:  31 percent:  0.34489351504542803\n",
      "height:  34 percent:  0.34840926055025717\n",
      "height:  76 percent:  0.35143922968193364\n",
      "height:  79 percent:  0.3710815812540307\n",
      "height:  82 percent:  0.3942285876347135\n",
      "height:  85 percent:  0.4046178243447732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 12/85 [03:12<19:22, 15.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 276\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_19.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_2.jpg\n",
      "height:  25 percent:  0.291907666076464\n",
      "height:  28 percent:  0.323891473551915\n",
      "height:  31 percent:  0.3594440276735851\n",
      "height:  34 percent:  0.37825288686329606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▌        | 13/85 [03:27<19:00, 15.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402 252\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_2.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_20.jpg\n",
      "height:  25 percent:  0.2688032002098498\n",
      "height:  31 percent:  0.2858705747865538\n",
      "height:  34 percent:  0.29920411691809184\n",
      "height:  37 percent:  0.33731457277294113\n",
      "height:  40 percent:  0.35379784546451215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▋        | 14/85 [03:43<18:50, 15.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height:  97 percent:  0.3691194220557062\n",
      "0 288\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_20.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_21.jpg\n",
      "height:  25 percent:  0.17757492294576693\n",
      "height:  28 percent:  0.20408903490397548\n",
      "height:  52 percent:  0.2204861111111111\n",
      "height:  55 percent:  0.23737358333622152\n",
      "height:  58 percent:  0.23748331878017034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|█▊        | 15/85 [03:59<18:26, 15.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270 321\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_21.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_22.jpg\n",
      "height:  25 percent:  0.18567906092202768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|█▉        | 16/85 [04:15<18:11, 15.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174 411\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_22.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_23.jpg\n",
      "height:  25 percent:  0.13521804708505478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 17/85 [04:31<18:01, 15.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261 345\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_23.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_24.jpg\n",
      "height:  25 percent:  0.12449340940389533\n",
      "height:  34 percent:  0.12674413053232045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|██        | 18/85 [04:47<17:43, 15.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 318\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_24.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_25.jpg\n",
      "height:  25 percent:  0.15402059151419764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██▏       | 19/85 [05:03<17:28, 15.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186 132\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_25.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_26.jpg\n",
      "height:  25 percent:  0.16921109580956128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|██▎       | 20/85 [05:19<17:21, 16.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 111\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_26.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_27.jpg\n",
      "height:  25 percent:  0.19420289855072465\n",
      "height:  28 percent:  0.22660573721694263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▍       | 21/85 [05:35<17:02, 15.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 312\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_27.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_28.jpg\n",
      "height:  25 percent:  0.20367761820447244\n",
      "height:  31 percent:  0.21343207825765964\n",
      "height:  34 percent:  0.24450220281504464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|██▌       | 22/85 [05:50<16:41, 15.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 300\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_28.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_29.jpg\n",
      "height:  25 percent:  0.22678208407108666\n",
      "height:  28 percent:  0.2684629462218596\n",
      "height:  31 percent:  0.273196275259291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 23/85 [06:06<16:28, 15.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 285\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_29.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_3.jpg\n",
      "height:  25 percent:  0.29579251098432685\n",
      "height:  31 percent:  0.33611155765131756\n",
      "height:  34 percent:  0.3450619159922011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██▊       | 24/85 [06:22<16:12, 15.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405 252\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_3.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_30.jpg\n",
      "height:  25 percent:  0.19893763525477082\n",
      "height:  28 percent:  0.20711088571869557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|██▉       | 25/85 [06:39<16:11, 16.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273 276\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_30.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_31.jpg\n",
      "height:  25 percent:  0.16469145517738867\n",
      "height:  28 percent:  0.17041981787312857\n",
      "height:  49 percent:  0.1739536723780421\n",
      "height:  52 percent:  0.18700815613925367\n",
      "height:  55 percent:  0.19876927875784148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 31%|███       | 26/85 [06:56<16:02, 16.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261 261\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_31.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_32.jpg\n",
      "height:  25 percent:  0.15448357269329138\n",
      "height:  28 percent:  0.17039005137477126\n",
      "height:  37 percent:  0.18024049280567364\n",
      "height:  40 percent:  0.2363003876892766\n",
      "height:  43 percent:  0.28286590941299383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|███▏      | 27/85 [07:12<15:49, 16.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237 252\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_32.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_33.jpg\n",
      "height:  25 percent:  0.1598504819988196\n",
      "height:  28 percent:  0.178611117235905\n",
      "height:  34 percent:  0.21278585003198214\n",
      "height:  37 percent:  0.25961084718023325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 28/85 [07:28<15:26, 16.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186 249\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_33.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_34.jpg\n",
      "height:  25 percent:  0.1892701160731851\n",
      "height:  28 percent:  0.22433135624986217\n",
      "height:  31 percent:  0.24776357779921174\n",
      "height:  37 percent:  0.2747528015820699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███▍      | 29/85 [07:44<15:09, 16.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 246\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_34.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_35.jpg\n",
      "height:  25 percent:  0.13855072463768117\n",
      "height:  31 percent:  0.17556323604710702\n",
      "height:  34 percent:  0.19680851063829788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|███▌      | 30/85 [08:01<14:51, 16.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 246\n",
      "saved /home/maciej/repos/pallet-recognition/data/jpeg_marked/localised/r_1_35.jpg\n",
      "/home/maciej/repos/pallet-recognition/data/jpeg_marked/ r_1_36.jpg\n",
      "height:  25 percent:  0.2702118171683389\n",
      "height:  28 percent:  0.32302383524794387\n",
      "height:  31 percent:  0.3909005525190821\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-310130a9bf5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlocalize_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-2580580e23a0>\u001b[0m in \u001b[0;36mlocalize_all\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mcenters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocalise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"eval/pred_centers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-fe38c7acd215>\u001b[0m in \u001b[0;36mlocalise\u001b[0;34m(path, filename)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#img_filtered = median_filter(img_classified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimg_rotated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_denoised\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mimg_pallet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_height\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_pallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rotated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mimg_full_marked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_pallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_full_marked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"localised/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-46b587925e2e>\u001b[0m in \u001b[0;36mfind_pallet\u001b[0;34m(img, min_height, max_height, step, min_perc)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_perc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmask_height\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mbest_for_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_pallet_by_height\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mperc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"height: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"percent: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-46b587925e2e>\u001b[0m in \u001b[0;36mfind_pallet_by_height\u001b[0;34m(img, mask_height)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mframe_mask_perc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmask_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mhole_1_mask_perc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhole_1_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mhole_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mhole_2_mask_perc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhole_2_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mhole_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mperc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_mask_perc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhole_1_mask_perc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhole_2_mask_perc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mperc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "localize_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
